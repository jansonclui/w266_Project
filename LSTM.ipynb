{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "import re\n",
    "import operator\n",
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189616\n",
      "22763\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#read in data to clean and separate into training and test set\n",
    "good = []\n",
    "bad = []\n",
    "score = []\n",
    "pre_tags = []\n",
    "tags = []\n",
    "pre_lines = []\n",
    "a = 0\n",
    "b = 0\n",
    "#1 is good\n",
    "#0 is not\n",
    "for line in open('listings_scores.csv').readlines():\n",
    "    data = line.split('\\t')\n",
    "    clean_score = re.sub(\"[^0-9]\", \"\", data[1])\n",
    "    desc = data[0]\n",
    "    if clean_score != '' and int(clean_score) <= 100 and int(clean_score) > 0:\n",
    "        if int(clean_score) >= 90:\n",
    "            for each in desc.lower().split('.'):\n",
    "                if each != ' ':\n",
    "                    good.append(re.sub('[^A-Za-z]', ' ', each))\n",
    "                    pre_lines.append((re.sub('[^A-Za-z]', ' ', each), 'good'))\n",
    "                    score.append(int(clean_score))\n",
    "                    tags.append(int(1))\n",
    "                    pre_tags.append(int(1))\n",
    "                    a += 1\n",
    "        elif int(clean_score) < 90:\n",
    "            for each in desc.lower().split('.'):\n",
    "                if each != ' ':\n",
    "                    pre_lines.append((re.sub('[^A-Za-z]', ' ', each), 'bad'))\n",
    "                    bad.append(re.sub('[^A-Za-z]', ' ', each))\n",
    "                    score.append(int(clean_score))\n",
    "                    pre_tags.append(int(0))\n",
    "                    b += 1\n",
    "\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189616\n",
      "189616\n",
      "189616\n",
      "379232\n",
      "379232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "lines_majority = good\n",
    "lines_minority = bad\n",
    " \n",
    "# Upsample minority class\n",
    "bad_upsampled = resample(lines_minority, \n",
    "                         replace=True,     # sample with replacement\n",
    "                         n_samples=189616,    # to match majority class\n",
    "                         random_state=123) # reproducible results\n",
    " \n",
    "    \n",
    "# Combine majority class with upsampled minority class\n",
    "print len(bad_upsampled)\n",
    "print len(good)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for each in good:\n",
    "    new_list.append((each, int(1)))\n",
    "\n",
    "print len(new_list)\n",
    "bad_upsampled = list(bad_upsampled)\n",
    "for each in bad_upsampled:\n",
    "    new_list.append((each, int(0)))\n",
    "\n",
    "shuffle(new_list)\n",
    "\n",
    "new_lines = []\n",
    "new_tags = []\n",
    "\n",
    "for each in new_list:\n",
    "    new_lines.append(each[0])\n",
    "    new_tags.append(each[1])\n",
    "                     \n",
    "                     \n",
    "print len(new_lines)\n",
    "print len(new_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379232\n",
      "[542]\n"
     ]
    }
   ],
   "source": [
    "def word_embedding(descriptions):\n",
    "    words = []\n",
    "    word_count = 0\n",
    "    for word in descriptions.split(\" \"):\n",
    "        words.append(word)\n",
    "    features = []\n",
    "    for each in words:\n",
    "        word_count += 1\n",
    "    \n",
    "    features.append(word_count)\n",
    "    return features\n",
    "\n",
    "\n",
    "featuresets2 = [word_embedding(descriptions) for (descriptions, tag) in new_list]\n",
    "print len(featuresets2)\n",
    "print max(featuresets2)\n",
    "#print new_list[0]\n",
    "#featuresets = [(word_embedding(descriptions), tag) for (descriptions, tag) in new_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97858\n",
      "143206\n",
      "77439\n",
      "31200\n",
      "13664\n",
      "6688\n",
      "3328\n",
      "1755\n",
      "1131\n",
      "770\n",
      "2193\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "under_10 = 0\n",
    "\n",
    "under_20 = 0\n",
    "\n",
    "under_30 = 0\n",
    "\n",
    "under_40 = 0\n",
    "\n",
    "under_50 = 0\n",
    "\n",
    "under_60 = 0\n",
    "\n",
    "under_70 = 0\n",
    "\n",
    "under_80 = 0\n",
    "\n",
    "under_90 = 0\n",
    "\n",
    "under_100 = 0\n",
    "\n",
    "over_100 = 0\n",
    "\n",
    "\n",
    "for each in featuresets2:\n",
    "    if each[0] <= 10:\n",
    "        under_10 += 1\n",
    "    elif each[0] <=20 and each[0] > 10:\n",
    "        under_20 += 1\n",
    "    elif each[0] <=30 and each[0] > 20:\n",
    "        under_30 += 1\n",
    "    elif each[0] <=40 and each[0] > 30:\n",
    "        under_40 += 1\n",
    "    elif each[0] <=50 and each[0] > 40:\n",
    "        under_50 += 1\n",
    "    elif each[0] <=60 and each[0] > 50:\n",
    "        under_60 += 1\n",
    "    elif each[0] <=70 and each[0] > 60:\n",
    "        under_70 += 1\n",
    "    elif each[0] <=80 and each[0] > 70:\n",
    "        under_80 += 1\n",
    "    elif each[0] <=90 and each[0] > 80:\n",
    "        under_90 += 1\n",
    "    elif each[0] <=100 and each[0] > 90:\n",
    "        under_100 += 1\n",
    "    elif each[0] > 100:\n",
    "        over_100 += 1\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    total += each[0]\n",
    "    count +=1\n",
    "#print total\n",
    "#print count\n",
    "\n",
    "print under_10 \n",
    "\n",
    "print under_20 \n",
    "\n",
    "print under_30 \n",
    "\n",
    "print under_40 \n",
    "\n",
    "print under_50 \n",
    "\n",
    "print under_60 \n",
    "\n",
    "print under_70 \n",
    "\n",
    "print under_80 \n",
    "\n",
    "print under_90\n",
    "\n",
    "print under_100\n",
    "\n",
    "print over_100\n",
    "\n",
    "\n",
    "#print total/count\n",
    "#print len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "feature_tags = []\n",
    "\n",
    "for each in featuresets:\n",
    "    sentences.append(each[0])\n",
    "    if each[1] == 1:\n",
    "        feature_tags.append(1)\n",
    "    elif each[1] == 0:\n",
    "        feature_tags.append(0)\n",
    "X_train = sentences[:len(sentences)/2]\n",
    "y_train = feature_tags[:len(feature_tags)/2]\n",
    "X_test = sentences[len(sentences)/2:]\n",
    "y_test = feature_tags[len(feature_tags)/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 25\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 20, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "189616/189616 [==============================] - 83s 439us/step - loss: 0.6638 - acc: 0.5910\n",
      "Epoch 2/5\n",
      "189616/189616 [==============================] - 86s 452us/step - loss: 0.6354 - acc: 0.6316\n",
      "Epoch 3/5\n",
      "189616/189616 [==============================] - 85s 449us/step - loss: 0.6175 - acc: 0.6491\n",
      "Epoch 4/5\n",
      "189616/189616 [==============================] - 85s 451us/step - loss: 0.6022 - acc: 0.6632\n",
      "Epoch 5/5\n",
      "189616/189616 [==============================] - 82s 431us/step - loss: 0.5838 - acc: 0.6792\n",
      "Accuracy: 67.66%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 25\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 25, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "189616/189616 [==============================] - 103s 543us/step - loss: 0.6647 - acc: 0.5864\n",
      "Epoch 2/5\n",
      "189616/189616 [==============================] - 97s 509us/step - loss: 0.6377 - acc: 0.62961s - loss: 0.\n",
      "Epoch 3/5\n",
      "189616/189616 [==============================] - 95s 503us/step - loss: 0.6190 - acc: 0.6473\n",
      "Epoch 4/5\n",
      "189616/189616 [==============================] - 97s 514us/step - loss: 0.5994 - acc: 0.6646\n",
      "Epoch 5/5\n",
      "189616/189616 [==============================] - 98s 518us/step - loss: 0.5795 - acc: 0.6816\n",
      "Accuracy: 68.57%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 20\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_56 (Embedding)     (None, 20, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_59 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "189616/189616 [==============================] - 99s 523us/step - loss: 0.6645 - recall: 0.6034 - precision: 0.5829 - f1_score: nan - acc: 0.5897\n",
      "Epoch 2/10\n",
      "189616/189616 [==============================] - 88s 466us/step - loss: 0.6384 - recall: 0.6390 - precision: 0.6283 - f1_score: 0.6273 - acc: 0.6289\n",
      "Epoch 3/10\n",
      "189616/189616 [==============================] - 97s 511us/step - loss: 0.6203 - recall: 0.6388 - precision: 0.6504 - f1_score: 0.6391 - acc: 0.64677s - loss: 0.6204 - recall: 0.6388 - precision: 0\n",
      "Epoch 4/10\n",
      "189616/189616 [==============================] - 92s 486us/step - loss: 0.6029 - recall: 0.6477 - precision: 0.6702 - f1_score: 0.6537 - acc: 0.6637\n",
      "Epoch 5/10\n",
      "189616/189616 [==============================] - 85s 448us/step - loss: 0.5842 - recall: 0.6553 - precision: 0.6901 - f1_score: 0.6676 - acc: 0.6799\n",
      "Epoch 6/10\n",
      "189616/189616 [==============================] - 86s 452us/step - loss: 0.5660 - recall: 0.6639 - precision: 0.7070 - f1_score: 0.6805 - acc: 0.6940\n",
      "Epoch 7/10\n",
      "189616/189616 [==============================] - 95s 501us/step - loss: 0.5496 - recall: 0.6728 - precision: 0.7222 - f1_score: 0.6923 - acc: 0.7066\n",
      "Epoch 8/10\n",
      "189616/189616 [==============================] - 93s 493us/step - loss: 0.5346 - recall: 0.6816 - precision: 0.7349 - f1_score: 0.7032 - acc: 0.71748s - l - ETA: 3s - loss: 0.5350 - recall: \n",
      "Epoch 9/10\n",
      "189616/189616 [==============================] - 97s 511us/step - loss: 0.5199 - recall: 0.6926 - precision: 0.7463 - f1_score: 0.7145 - acc: 0.7285\n",
      "Epoch 10/10\n",
      "189616/189616 [==============================] - 97s 511us/step - loss: 0.5053 - recall: 0.7014 - precision: 0.7592 - f1_score: 0.7253 - acc: 0.7391\n",
      "Accuracy: 68.55%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[recall, precision,f1_score, 'accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 25\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_57 (Embedding)     (None, 25, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "189616/189616 [==============================] - 112s 588us/step - loss: 0.6637 - recall: 0.6081 - precision: 0.5867 - f1_score: nan - acc: 0.5910 - loss: 0.6638 - recall: 0.6077 - precision: 0.5865 - f1_score: nan -\n",
      "Epoch 2/10\n",
      "189616/189616 [==============================] - 98s 519us/step - loss: 0.6380 - recall: 0.6428 - precision: 0.6271 - f1_score: 0.6287 - acc: 0.62901s - loss: 0.6383 - recall: 0.6426 - precision: 0.6268 - f1_score:\n",
      "Epoch 3/10\n",
      "189616/189616 [==============================] - 106s 559us/step - loss: 0.6205 - recall: 0.6450 - precision: 0.6463 - f1_score: 0.6402 - acc: 0.6452\n",
      "Epoch 4/10\n",
      "189616/189616 [==============================] - 97s 513us/step - loss: 0.6012 - recall: 0.6436 - precision: 0.6726 - f1_score: 0.6528 - acc: 0.6642\n",
      "Epoch 5/10\n",
      "189616/189616 [==============================] - 103s 541us/step - loss: 0.5808 - recall: 0.6508 - precision: 0.6920 - f1_score: 0.6659 - acc: 0.6798\n",
      "Epoch 6/10\n",
      "189616/189616 [==============================] - 105s 553us/step - loss: 0.5620 - recall: 0.6607 - precision: 0.7098 - f1_score: 0.6798 - acc: 0.6944\n",
      "Epoch 7/10\n",
      "189616/189616 [==============================] - 116s 611us/step - loss: 0.5436 - recall: 0.6725 - precision: 0.7281 - f1_score: 0.6951 - acc: 0.7102\n",
      "Epoch 8/10\n",
      "189616/189616 [==============================] - 105s 554us/step - loss: 0.5278 - recall: 0.6831 - precision: 0.7422 - f1_score: 0.7074 - acc: 0.7226\n",
      "Epoch 9/10\n",
      "189616/189616 [==============================] - 104s 549us/step - loss: 0.5119 - recall: 0.6937 - precision: 0.7556 - f1_score: 0.7193 - acc: 0.7344\n",
      "Epoch 10/10\n",
      "189616/189616 [==============================] - 94s 498us/step - loss: 0.4990 - recall: 0.7015 - precision: 0.7653 - f1_score: 0.7282 - acc: 0.7431\n",
      "Accuracy: 67.22%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[recall, precision,f1_score, 'accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def mcor(y_true, y_pred):\n",
    "    #matthews_correlation\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    " \n",
    " \n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "#you can use it like this\n",
    "#model.compile(loss='binary_crossentropy',\n",
    "#              optimizer= \"adam\",\n",
    "#              metrics=[mcor,recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 30\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_54 (Embedding)     (None, 30, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "189616/189616 [==============================] - 125s 661us/step - loss: 0.6857 - recall: 0.5546 - precision: 0.5251 - f1_score: nan - acc: 0.5400\n",
      "Epoch 2/10\n",
      "189616/189616 [==============================] - 113s 598us/step - loss: 0.6644 - recall: 0.6062 - precision: 0.5947 - f1_score: 0.5908 - acc: 0.5932\n",
      "Epoch 3/10\n",
      "189616/189616 [==============================] - 118s 625us/step - loss: 0.6520 - recall: 0.6244 - precision: 0.6122 - f1_score: 0.6109 - acc: 0.6125\n",
      "Epoch 4/10\n",
      "189616/189616 [==============================] - 116s 611us/step - loss: 0.6443 - recall: 0.6311 - precision: 0.6220 - f1_score: 0.6199 - acc: 0.6224\n",
      "Epoch 5/10\n",
      "189616/189616 [==============================] - 113s 595us/step - loss: 0.6391 - recall: 0.6364 - precision: 0.6275 - f1_score: 0.6256 - acc: 0.6285\n",
      "Epoch 6/10\n",
      "189616/189616 [==============================] - 119s 630us/step - loss: 0.6350 - recall: 0.6384 - precision: 0.6351 - f1_score: 0.6304 - acc: 0.6344\n",
      "Epoch 7/10\n",
      "189616/189616 [==============================] - 118s 623us/step - loss: 0.6307 - recall: 0.6420 - precision: 0.6404 - f1_score: 0.6350 - acc: 0.6389\n",
      "Epoch 8/10\n",
      "189616/189616 [==============================] - 123s 646us/step - loss: 0.6269 - recall: 0.6445 - precision: 0.6439 - f1_score: 0.6383 - acc: 0.6430\n",
      "Epoch 9/10\n",
      "189616/189616 [==============================] - 125s 658us/step - loss: 0.6238 - recall: 0.6447 - precision: 0.6487 - f1_score: 0.6407 - acc: 0.6459\n",
      "Epoch 10/10\n",
      "189616/189616 [==============================] - 122s 642us/step - loss: 0.6206 - recall: 0.6453 - precision: 0.6520 - f1_score: 0.6428 - acc: 0.6494\n",
      "Accuracy: 61.63%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=[recall, precision,f1_score, 'accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 40\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 40, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "189616/189616 [==============================] - 154s 814us/step - loss: 0.6877 - recall: 0.5017 - precision: 0.5150 - f1_score: nan - acc: 0.5356\n",
      "Epoch 2/10\n",
      "189616/189616 [==============================] - 138s 726us/step - loss: 0.6669 - recall: 0.5993 - precision: 0.5907 - f1_score: 0.5860 - acc: 0.5898\n",
      "Epoch 3/10\n",
      "189616/189616 [==============================] - 131s 693us/step - loss: 0.6548 - recall: 0.6166 - precision: 0.6086 - f1_score: 0.6049 - acc: 0.6080\n",
      "Epoch 4/10\n",
      "189616/189616 [==============================] - 144s 759us/step - loss: 0.6459 - recall: 0.6304 - precision: 0.6185 - f1_score: 0.6175 - acc: 0.6193\n",
      "Epoch 5/10\n",
      "189616/189616 [==============================] - 146s 769us/step - loss: 0.6404 - recall: 0.6365 - precision: 0.6268 - f1_score: 0.6250 - acc: 0.6271\n",
      "Epoch 6/10\n",
      "189616/189616 [==============================] - 128s 677us/step - loss: 0.6361 - recall: 0.6369 - precision: 0.6319 - f1_score: 0.6284 - acc: 0.6316\n",
      "Epoch 7/10\n",
      "189616/189616 [==============================] - 146s 771us/step - loss: 0.6324 - recall: 0.6412 - precision: 0.6376 - f1_score: 0.6335 - acc: 0.6373\n",
      "Epoch 8/10\n",
      "189616/189616 [==============================] - 131s 692us/step - loss: 0.6282 - recall: 0.6425 - precision: 0.6423 - f1_score: 0.6366 - acc: 0.6409\n",
      "Epoch 9/10\n",
      "189616/189616 [==============================] - 129s 682us/step - loss: 0.6247 - recall: 0.6435 - precision: 0.6470 - f1_score: 0.6400 - acc: 0.6454\n",
      "Epoch 10/10\n",
      "189616/189616 [==============================] - 132s 696us/step - loss: 0.6220 - recall: 0.6429 - precision: 0.6502 - f1_score: 0.6412 - acc: 0.6475\n",
      "Accuracy: 66.28%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=[recall, precision,f1_score, 'accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 50\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 50, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "189616/189616 [==============================] - 166s 876us/step - loss: 0.6636 - acc: 0.5910\n",
      "Epoch 2/5\n",
      "189616/189616 [==============================] - 156s 823us/step - loss: 0.6358 - acc: 0.6308\n",
      "Epoch 3/5\n",
      "189616/189616 [==============================] - 159s 840us/step - loss: 0.6176 - acc: 0.6505\n",
      "Epoch 4/5\n",
      "189616/189616 [==============================] - 161s 848us/step - loss: 0.5975 - acc: 0.6677\n",
      "Epoch 5/5\n",
      "189616/189616 [==============================] - 155s 820us/step - loss: 0.5785 - acc: 0.6818\n",
      "Accuracy: 68.27%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 60\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 60, 32)            718976    \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "189616/189616 [==============================] - 179s 942us/step - loss: 0.6637 - acc: 0.5890\n",
      "Epoch 2/5\n",
      "189616/189616 [==============================] - 175s 921us/step - loss: 0.6370 - acc: 0.6310\n",
      "Epoch 3/5\n",
      "189616/189616 [==============================] - 175s 920us/step - loss: 0.6182 - acc: 0.6482\n",
      "Epoch 4/5\n",
      "189616/189616 [==============================] - 175s 921us/step - loss: 0.5982 - acc: 0.6657\n",
      "Epoch 5/5\n",
      "189616/189616 [==============================] - 174s 920us/step - loss: 0.5787 - acc: 0.6801\n",
      "Accuracy: 68.26%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 100\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 100, 32)           718976    \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "189616/189616 [==============================] - 272s 1ms/step - loss: 0.6657 - acc: 0.5868\n",
      "Epoch 2/5\n",
      "189616/189616 [==============================] - 270s 1ms/step - loss: 0.6421 - acc: 0.6229\n",
      "Epoch 3/5\n",
      "189616/189616 [==============================] - 269s 1ms/step - loss: 0.6228 - acc: 0.6437\n",
      "Epoch 4/5\n",
      "189616/189616 [==============================] - 270s 1ms/step - loss: 0.6037 - acc: 0.6594\n",
      "Epoch 5/5\n",
      "189616/189616 [==============================] - 279s 1ms/step - loss: 0.5827 - acc: 0.6766\n",
      "Accuracy: 68.25%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005509\n",
      "[('room', 22467), ('san', 16133), ('na', 15992), ('francisco', 15565), ('street', 15351), ('bedroom', 14265), ('kitchen', 13869), ('home', 13798), ('house', 12420), ('apartment', 12390)]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for text, rating in lines:\n",
    "    for word in text.split():\n",
    "        if word.lower() not in stop_words:\n",
    "            words.append(word.lower())\n",
    "\n",
    "print len(words)\n",
    "word_features = nltk.FreqDist(words)\n",
    "most_common = word_features.most_common()[:10000]\n",
    "print most_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_common2 = word_features.most_common()\n",
    "print len(most_common2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plain_words = []\n",
    "embedded_words = []\n",
    "word_dict = {}\n",
    "for each in most_common:\n",
    "    plain_words.append(each[0])\n",
    "    embedded_words.append(each[1])\n",
    "    word_dict[each[0]] = each[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedroom\n",
      "14265\n",
      "14265\n"
     ]
    }
   ],
   "source": [
    "print plain_words[5]\n",
    "print embedded_words[5]\n",
    "print word_dict['bedroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_embedding(descriptions):\n",
    "    words = []\n",
    "    for word in descriptions.split(\" \"):\n",
    "        words.append(word)\n",
    "    features = []\n",
    "    for each in words:\n",
    "        if each in plain_words:\n",
    "            features.append(word_dict[each])\n",
    "        elif each not in just_words:\n",
    "            features.append(0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = lines[:10]\n",
    "featuresets = [(word_embedding(descriptions), tag) for (descriptions, tag) in test]\n",
    "\n",
    "sentences = []\n",
    "feature_tags = []\n",
    "\n",
    "for each in featuresets:\n",
    "    sentences.append(each[0])\n",
    "    if each[1] == 'good':\n",
    "        feature_tags.append(each[1])\n",
    "    elif each[1] == 'bad':\n",
    "        feature_tags.append(each[0])\n",
    "    \n",
    "X_train = sentences[:5]\n",
    "y_train = feature_tags[:5]\n",
    "\n",
    "X_test = sentences[5:]\n",
    "y_test = feature_tags[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "#from keras.datasets import imdb\n",
    "\n",
    "## load the dataset but only keep the top n words, zero the rest\n",
    "#top_words = 5000\n",
    "#(X1_train, y1_train), (X1_test, y1_test) = imdb.load_data(num_words=top_words)\n",
    "print (X1_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print X1_train[0]\n",
    "print y1_train[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def feature_sep(descriptions):\n",
    "#    words = []\n",
    "##    for word in descriptions.split(\" \"):\n",
    " #       words.append(word)\n",
    " #   features = []\n",
    "#    for each in words:\n",
    "#        if each in just_words:\n",
    "#            features.append(1)\n",
    "#        elif each not in just_words:\n",
    "##            features.append(0)\n",
    "#    return features\n",
    "def word_embedding(descriptions):\n",
    "    words = []\n",
    "    for word in descriptions.split(\" \"):\n",
    "        words.append(word)\n",
    "    features = []\n",
    "    for each in words:\n",
    "        if each in plain_words:\n",
    "            features.append(word_dict[each])\n",
    "        elif each not in just_words:\n",
    "            features.append(0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(word_embedding(descriptions), tag) for (descriptions, tag) in lines]\n",
    "\n",
    "sentences = []\n",
    "feature_tags = []\n",
    "\n",
    "for each in featuresets:\n",
    "    sentences.append(each[0])\n",
    "    if each[1] == 'good':\n",
    "        feature_tags.append(1)\n",
    "    elif each[1] == 'bad':\n",
    "        feature_tags.append(0)\n",
    "    \n",
    "X_train = sentences[:len(sentences)/2]\n",
    "y_train = feature_tags[:len(feature_tags)/2]\n",
    "X_test = sentences[len(sentences)/2:]\n",
    "y_test = feature_tags[len(feature_tags)/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 800\n",
    "X_pad_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_pad_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print len(X_pad_train[0])\n",
    "print y_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 1000, 32)          718976    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3502 samples, validate on 3502 samples\n",
      "Epoch 1/3\n",
      "3502/3502 [==============================] - 56s 16ms/step - loss: 0.2258 - acc: 0.9654 - val_loss: 0.0919 - val_acc: 0.9817\n",
      "Epoch 2/3\n",
      "3502/3502 [==============================] - 55s 16ms/step - loss: 0.1142 - acc: 0.9760 - val_loss: 0.0921 - val_acc: 0.9817\n",
      "Epoch 3/3\n",
      "3502/3502 [==============================] - 55s 16ms/step - loss: 0.1143 - acc: 0.9760 - val_loss: 0.0914 - val_acc: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a356b2bd0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "top_words = 22468\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, validation_data=(X_pad_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 800, 32)           718976    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 772,277\n",
      "Trainable params: 772,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "3502/3502 [==============================] - 46s 13ms/step - loss: 0.2299 - acc: 0.9617\n",
      "Epoch 2/3\n",
      "3502/3502 [==============================] - 45s 13ms/step - loss: 0.1148 - acc: 0.9760\n",
      "Epoch 3/3\n",
      "3502/3502 [==============================] - 44s 13ms/step - loss: 0.1137 - acc: 0.9760\n",
      "Accuracy: 98.17%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "top_words = 22468\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_pad_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_pad_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
